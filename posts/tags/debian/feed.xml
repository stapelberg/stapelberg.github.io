<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Michael Stapelbergs Website: posts tagged debian</title>
  <link href="https://michael.stapelberg.ch/posts/tags/debian/feed.xml" rel="self"/>
  <link href="https://michael.stapelberg.ch/posts/tags/debian/"/>
  <updated>2019-07-20T00:00:00+00:00</updated>
  <id>https://michael.stapelberg.ch/posts/tags/debian/</id>
  <generator>Hugo -- gohugo.io</generator>
  <entry>
    <title type="html"><![CDATA[Linux distributions: Can we do without hooks and triggers?]]></title>
    <link href="https://michael.stapelberg.ch/posts/2019-07-20-hooks-and-triggers/"/>
    <id>https://michael.stapelberg.ch/posts/2019-07-20-hooks-and-triggers/</id>
    <published>2019-07-20T00:00:00+00:00</published>
    <updated>2019-07-20T18:06:33+02:00</updated>
    <content type="html"><![CDATA[

<p>Hooks are an extension feature provided by all package managers that are used in
larger Linux distributions. For example, Debian uses apt, which has various
<a href="https://www.debian.org/doc/debian-policy/ap-flowcharts.html">maintainer
scripts</a>. Fedora
uses rpm, which has
<a href="https://fedoraproject.org/wiki/Packaging:Scriptlets">scriptlets</a>. Different
package managers use different names for the concept, but all of them offer
package maintainers the ability to run arbitrary code during package
installation and upgrades. Example hook use cases include adding daemon user
accounts to your system (e.g. <code>postgres</code>), or generating/updating cache files.</p>

<p>Triggers are a kind of hook which run when <em>other</em> packages are installed. For
example, on Debian, the <a href="https://manpages.debian.org/man.1"><code>man(1)</code></a> package
comes with a trigger which regenerates the search database index whenever any
package installs a manpage. When, for example, the
<a href="https://manpages.debian.org/nginx.8"><code>nginx(8)</code></a> package is installed, a
trigger provided by the <a href="https://manpages.debian.org/man.1"><code>man(1)</code></a> package
runs.</p>

<p>Over the past few decades, Open Source software has become more and more
uniform: instead of each piece of software defining its own rules, a small
number of build systems are now widely adopted.</p>

<p>Hence, I think it makes sense to revisit whether offering extension via hooks
and triggers is a net win or net loss.</p>

<h3 id="hooks-preclude-concurrent-package-installation">Hooks preclude concurrent package installation</h3>

<p>Package managers commonly can make very little assumptions about what hooks do,
what preconditions they require, and which conflicts might be caused by running
multiple package’s hooks concurrently.</p>

<p>Hence, package managers cannot concurrently install packages. At least the
hook/trigger part of the installation needs to happen in sequence.</p>

<p>While it seems technically feasible to retrofit package manager hooks with
concurrency primitives such as locks for mutual exclusion between different hook
processes, the required overhaul of all hooks¹ seems like such a daunting task
that it might be better to just get rid of the hooks instead. Only deleting code
frees you from the burden of maintenance, automated testing and debugging.</p>

<p>① In Debian, there are 8620 non-generated maintainer scripts, as reported by
   <code>find shard*/src/*/debian -regex &quot;.*\(pre\|post\)\(inst\|rm\)$&quot;</code> on a Debian
   Code Search instance.</p>

<h3 id="triggers-slow-down-installing-updating-other-packages">Triggers slow down installing/updating other packages</h3>

<p>Personally, I never use the
<a href="https://manpages.debian.org/apropos.1"><code>apropos(1)</code></a> command, so I don’t
appreciate the <a href="https://manpages.debian.org/man.1"><code>man(1)</code></a> package’s trigger
which updates the database used by
<a href="https://manpages.debian.org/apropos.1"><code>apropos(1)</code></a>. The process takes a long
time and, because hooks and triggers must be executed serially (see previous
section), blocks my installation or update.</p>

<p>When I tell people this, they are often surprised to learn about the existance
of the <a href="https://manpages.debian.org/apropos.1"><code>apropos(1)</code></a> command. I suggest
adopting an opt-in model.</p>

<h3 id="unnecessary-work-if-programs-are-not-used-between-updates">Unnecessary work if programs are not used between updates</h3>

<p>Hooks run when packages are installed. If a package’s contents are not used
between two updates, running the hook in the first update could have been
skipped. Running the hook lazily when the package contents are used reduces
unnecessary work.</p>

<p>As a welcome side-effect, lazy hook evaluation automatically makes the hook work
in operating system images, such as live USB thumb drives or SD card images for
the Raspberry Pi. Such images must not ship the same crypto keys (e.g. OpenSSH
host keys) to all machines, but instead generate a different key on each
machine.</p>

<p>Why do users keep packages installed they don’t use? It’s extra work to remember
and clean up those packages after use. Plus, users might not realize or value
that having fewer packages installed has benefits such as faster updates.</p>

<p>I can also imagine that there are people for whom the cost of re-installing
packages incentivizes them to just keep packages installed—you never know when
you might need the program again…</p>

<h3 id="implemented-in-an-interpreted-language">Implemented in an interpreted language</h3>

<p>While working on hermetic packages (more on that in another blog post), where
the contained programs are started with modified environment variables
(e.g. <code>PATH</code>) via a wrapper bash script, I noticed that the overhead of those
wrapper bash scripts quickly becomes significant. For example, when using the
excellent <a href="https://magit.vc/">magit</a> interface for Git in Emacs, I encountered
second-long delays² when using hermetic packages compared to standard
packages. Re-implementing wrappers in a compiled language provided a significant
speed-up.</p>

<p>Similarly, getting rid of an extension point which mandates using shell scripts
allows us to build an efficient and fast implementation of a predefined set of
primitives, where you can reason about their effects and interactions.</p>

<p>② magit needs to run git a few times for displaying the full status, so small
   overhead quickly adds up.</p>

<h3 id="incentivizing-more-upstream-standardization">Incentivizing more upstream standardization</h3>

<p>Hooks are an escape hatch for distribution maintainers to express anything which
their packaging system cannot express.</p>

<p>Distributions should only rely on well-established interfaces such as autoconf’s
classic <code>./configure &amp;&amp; make &amp;&amp; make install</code> (including commonly used flags) to
build a distribution package. Integrating upstream software into a distribution
should not require custom hooks. For example, instead of requiring a hook which
updates a cache of schema files, the library used to interact with those files
should transparently (re-)generate the cache or fall back to a slower code path.</p>

<p>Distribution maintainers are hard to come by, so we should value their time. In
particular, there is a 1:n relationship of packages to distribution package
maintainers (software is typically available in multiple Linux distributions),
so it makes sense to spend the work in the 1 and have the n benefit.</p>

<h3 id="can-we-do-without-them">Can we do without them?</h3>

<p>If we want to get rid of hooks, we need another mechanism to achieve what we
currently achieve with hooks.</p>

<p>If the hook is not specific to the package, it can be moved to the package
manager. The desired system state should either be derived from the package
contents (e.g. required system users can be discovered from systemd service
files) or declaratively specified in the package build instructions—more on that
in another blog post. This turns hooks (arbitrary code) into configuration,
which allows the package manager to collapse and sequence the required state
changes. E.g., when 5 packages are installed which each need a new system user,
the package manager could update <code>/etc/passwd</code> just once.</p>

<p>If the hook is specific to the package, it should be moved into the package
contents. This typically means moving the functionality into the program start
(or the systemd service file if we are talking about a daemon). If (while?)
upstream is not convinced, you can either wrap the program or patch it. Note
that this case is relatively rare: I have worked with hundreds of packages and
the only package-specific functionality I came across was automatically
generating host keys before starting OpenSSH’s
<a href="https://manpages.debian.org/sshd.8"><code>sshd(8)</code></a>³.</p>

<p>There is one exception where moving the hook doesn’t work: packages which modify
state outside of the system, such as bootloaders or kernel images.</p>

<p>③ Even that can be moved out of a package-specific hook, <a href="https://src.fedoraproject.org/rpms/openssh/blob/30922f629cc135e3233e263d5e3eb346f9251c4e/f/sshd-keygen%40.service">as Fedora
demonstrates</a>.</p>

<h3 id="conclusion">Conclusion</h3>

<p>Global state modifications performed as part of package installation today use
hooks, an overly expressive extension mechanism.</p>

<p>Instead, all modifications should be driven by configuration. This is feasible
because there are only a few different kinds of desired state
modifications. This makes it possible for package managers to optimize package
installation.</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Optional dependencies don’t work]]></title>
    <link href="https://michael.stapelberg.ch/posts/2019-05-23-optional-dependencies/"/>
    <id>https://michael.stapelberg.ch/posts/2019-05-23-optional-dependencies/</id>
    <published>2019-05-23T00:00:00+00:00</published>
    <updated>2019-05-23T14:55:17+02:00</updated>
    <content type="html"><![CDATA[

<p>In the i3 projects, we have always tried hard to avoid optional
dependencies. There are a number of reasons behind it, and as I have recently
encountered some of the downsides of optional dependencies firsthand, I
summarized my thoughts in this article.</p>

<h3 id="what-is-a-compile-time-optional-dependency">What is a (compile-time) optional dependency?</h3>

<p>When building software from source, most programming languages and build systems
support conditional compilation: different parts of the source code are compiled
based on certain conditions.</p>

<p>An optional dependency is conditional compilation hooked up directly to a knob
(e.g. command line flag, configuration file, …), with the effect that the
software can now be built without an otherwise required dependency.</p>

<p>Let’s walk through a few issues with optional dependencies.</p>

<h3 id="inconsistent-experience-in-different-environments">Inconsistent experience in different environments</h3>

<p>Software is usually not built by end users, but by packagers, at least when we
are talking about Open Source.</p>

<p>Hence, end users don’t see the knob for the optional dependency, they are just
presented with the fait accompli: their version of the software behaves
differently than other versions of the same software.</p>

<p>Depending on the kind of software, this situation can be made obvious to the
user: for example, if the optional dependency is needed to print documents, the
program can produce an appropriate error message when the user tries to print a
document.</p>

<p>Sometimes, this isn’t possible: when i3 introduced an optional dependency on
cairo and pangocairo, the behavior itself (rendering window titles) worked in
all configurations, but non-ASCII characters might break depending on whether i3
was compiled with cairo.</p>

<p>For users, it is frustrating to only discover in conversation that a program has
a feature that the user is interested in, but it’s not available on their
computer. For support, this situation can be hard to detect, and even harder to
resolve to the user’s satisfaction.</p>

<h3 id="packaging-is-more-complicated">Packaging is more complicated</h3>

<p>Unfortunately, many build systems don’t stop the build when optional
dependencies are not present. Instead, you sometimes end up with a broken build,
or, even worse: with a successful build that does not work correctly at runtime.</p>

<p>This means that packagers need to closely examine the build output to know which
dependencies to make available. In the best case, there is a summary of
available and enabled options, clearly outlining what this build will
contain. In the worst case, you need to infer the features from the checks that
are done, or work your way through the <code>--help</code> output.</p>

<p>The better alternative is to configure your build system such that it stops when
<em>any</em> dependency was not found, and thereby have packagers acknowledge each
optional dependency by explicitly disabling the option.</p>

<h3 id="untested-code-paths-bit-rot">Untested code paths bit rot</h3>

<p>Code paths which are not used will inevitably bit rot. If you have optional
dependencies, you need to test both the code path without the dependency and the
code path with the dependency. It doesn’t matter whether the tests are automated
or manual, the test matrix must cover both paths.</p>

<p>Interestingly enough, this principle seems to apply to all kinds of software
projects (but it slows down as change slows down): one might think that
important Open Source building blocks should have enough users to cover all
sorts of configurations.</p>

<p>However, consider this example: building cairo without libxrender results in all
GTK application windows, menus, etc. being displayed as empty grey
surfaces. Cairo does not fail to build without libxrender, but the code path
clearly is broken without libxrender.</p>

<h3 id="can-we-do-without-them">Can we do without them?</h3>

<p>I’m not saying optional dependencies should <em>never</em> be used. In fact, for
bootstrapping, disabling dependencies can save a lot of work and can sometimes
allow breaking circular dependencies. For example, in an early bootstrapping
stage, binutils can be compiled with <code>--disable-nls</code> to disable
internationalization.</p>

<p>However, optional dependencies are broken so often that I conclude they are
overused. Read on and see for yourself whether you would rather commit to best
practices or not introduce an optional dependency.</p>

<h3 id="best-practices">Best practices</h3>

<p>If you do decide to make dependencies optional, please:</p>

<ol>
<li>Set up automated testing for <strong>all</strong> code path combinations.</li>
<li>Fail the build until packagers explicitly pass a <code>--disable</code> flag.</li>
<li>Tell users their version is missing a dependency at runtime, e.g. in <code>--version</code>.</li>
</ol>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Winding down my Debian involvement]]></title>
    <link href="https://michael.stapelberg.ch/posts/2019-03-10-debian-winding-down/"/>
    <id>https://michael.stapelberg.ch/posts/2019-03-10-debian-winding-down/</id>
    <published>2019-03-10T00:00:00+00:00</published>
    <updated>2019-03-10T21:43:19+01:00</updated>
    <content type="html"><![CDATA[

<p>This post is hard to write, both in the emotional sense but also in the “I would
have written a shorter letter, but I didn’t have the time” sense. Hence, please
assume the best of intentions when reading it—it is not my intention to make
anyone feel bad about their contributions, but rather to provide some insight
into why my frustration level ultimately exceeded the threshold.</p>

<p>Debian has been in my life for well over 10 years at this point.</p>

<p>A few weeks ago, I have visited some old friends at the Zürich Debian meetup
after a multi-year period of absence. On my bike ride home, it occurred to me
that the topics of our discussions had remarkable overlap with my last visit. We
had a discussion about the merits of systemd, which took a detour to respect in
open source communities, returned to processes in Debian and eventually
culminated in democracies and their theoretical/practical failings. Admittedly,
that last one might be a Swiss thing.</p>

<p>I say this not to knock on the Debian meetup, but because it prompted me to
reflect on what feelings Debian is invoking lately and whether it’s still a good
fit for me.</p>

<p>So I’m finally making a decision that I should have made a long time ago: I am
winding down my involvement in Debian to a minimum.</p>

<h3 id="what-does-this-mean">What does this mean?</h3>

<p>Over the coming weeks, I will:</p>

<ul>
<li>transition packages to be team-maintained where it makes sense</li>
<li>remove myself from the <code>Uploaders</code> field on packages with other maintainers</li>
<li>orphan packages where I am the sole maintainer</li>
</ul>

<p>I will try to keep up best-effort maintenance of the
<a href="https://manpages.debian.org/">manpages.debian.org</a> service and the
<a href="https://codesearch.debian.net/">codesearch.debian.net</a> service, but any help
would be much appreciated.</p>

<p>For all intents and purposes, please treat me as permanently on vacation. I will
try to be around for administrative issues (e.g. permission transfers) and
questions addressed directly to me, permitted they are easy enough to answer.</p>

<h3 id="why">Why?</h3>

<p>When I joined Debian, I was still studying, i.e. I had luxurious amounts of
spare time. Now, over 5 years of full time work later, my day job taught me a
lot, both about what works in large software engineering projects and how I
personally like my computer systems. I am very conscious of how I spend the
little spare time that I have these days.</p>

<p>The following sections each deal with what I consider a major pain point, in no
particular order. Some of them influence each other—for example, if changes
worked better, we could have a chance at transitioning packages to be more
easily machine readable.</p>

<h4 id="change-process-in-debian">Change process in Debian</h4>

<p>The last few years, my current team at work conducted various smaller and larger
refactorings across the entire code base (touching thousands of projects), so we
have learnt a lot of valuable lessons about how to effectively do these
changes. It irks me that Debian works almost the opposite way in every regard. I
appreciate that every organization is different, but I think a lot of my points
do actually apply to Debian.</p>

<p>In Debian, packages are nudged in the right direction by a document called the
<a href="https://www.debian.org/doc/debian-policy/">Debian Policy</a>, or its programmatic
embodiment, lintian.</p>

<p>While it is great to have a lint tool (for quick, local/offline feedback), it is
even better to not require a lint tool at all. The team conducting the change
(e.g. the C++ team introduces a new hardening flag for all packages) should be
able to do their work transparent to me.</p>

<p>Instead, currently, all packages become lint-unclean, all maintainers need to
read up on what the new thing is, how it might break, whether/how it affects
them, manually run some tests, and finally decide to opt in. This causes a lot
of overhead and manually executed mechanical changes across packages.</p>

<p>Notably, the <strong>cost of each change</strong> is distributed onto the package maintainers in
the Debian model. At work, we have found that the opposite works better: if the
team behind the change is put in power to do the change for as many users as
possible, they can be significantly more efficient at it, which reduces the
total cost and time a lot. Of course, exceptions (e.g. a large project abusing a
language feature) should still be taken care of by the respective owners, but
the important bit is that the default should be the other way around.</p>

<p>Debian is <strong>lacking tooling for large changes</strong>: it is hard to programmatically
deal with packages and repositories (see the section below). The closest to
“sending out a change for review” is to open a bug report with an attached
patch. I thought the workflow for accepting a change from a bug report was too
complicated and started <a href="/posts/2016-07-17-mergebot/">mergebot</a>, but only Guido
ever signaled interest in the project.</p>

<p>Culturally, reviews and reactions are slow. There are no deadlines. I literally
sometimes get emails notifying me that a patch I sent out a few years ago (!!)
is now merged. This turns projects from a small number of weeks into many years,
which is a huge demotivator for me.</p>

<p>Interestingly enough, you can see artifacts of the slow online activity manifest
itself in the offline culture as well: I don’t want to be discussing systemd’s
merits 10 years after I first heard about it.</p>

<p>Lastly, changes can easily be slowed down significantly by holdouts who refuse
to collaborate. My canonical example for this is rsync, whose maintainer refused
my patches to make the package use debhelper purely out of personal preference.</p>

<p>Granting so much personal freedom to individual maintainers prevents us as a
project from raising the abstraction level for building Debian packages, which
in turn makes tooling harder.</p>

<p>How would things look like in a better world?</p>

<ol>
<li>As a project, we should strive towards more unification. Uniformity still
does not rule out experimentation, it just changes the trade-off from easier
experimentation and harder automation to harder experimentation and easier
automation.</li>
<li>Our culture needs to shift from “this package is my domain, how dare you
touch it” to a shared sense of ownership, where anyone in the project can
easily contribute (reviewed) changes without necessarily even involving
individual maintainers.</li>
</ol>

<p>To learn more about how successful large changes can look like, I recommend <a href="https://www.youtube.com/watch?v=TrC6ROeV4GI">my
colleague Hyrum Wright’s talk “Large-Scale Changes at Google: Lessons Learned
From 5 Yrs of Mass Migrations”</a>.</p>

<h4 id="fragmented-workflow-and-infrastructure">Fragmented workflow and infrastructure</h4>

<p>Debian generally seems to prefer decentralized approaches over centralized
ones. For example, individual packages are maintained in separate repositories
(as opposed to in one repository), each repository can use any SCM (git and svn
are common ones) or no SCM at all, and each repository can be hosted on a
different site. Of course, what you do in such a repository also varies subtly
from team to team, and even within teams.</p>

<p>In practice, non-standard hosting options are used rarely enough to not justify
their cost, but frequently enough to be a huge pain when trying to automate
changes to packages. Instead of using GitLab’s API to create a merge request,
you have to design an entirely different, more complex system, which deals with
intermittently (or permanently!) unreachable repositories and abstracts away
differences in patch delivery (bug reports, merge requests, pull requests,
email, …).</p>

<p>Wildly diverging workflows is not just a temporary problem either. I
participated in long discussions about different git workflows during DebConf
13, and gather that there were similar discussions in the meantime.</p>

<p>Personally, I cannot keep enough details of the different workflows in my
head. Every time I touch a package that works differently than mine, it
frustrates me immensely to re-learn aspects of my day-to-day.</p>

<p>After noticing workflow fragmentation in the Go packaging team (which I
started), I tried fixing this with the <a href="https://go-team.pages.debian.net/workflow-changes.html">workflow changes
proposal</a>, but did not
succeed in implementing it. The lack of effective automation and slow pace of
changes in the surrounding tooling despite my willingness to contribute time and
energy killed any motivation I had.</p>

<h4 id="old-infrastructure-package-uploads">Old infrastructure: package uploads</h4>

<p>When you want to make a package available in Debian, you upload GPG-signed files
via anonymous FTP. There are several batch jobs (the queue daemon, <code>unchecked</code>,
<code>dinstall</code>, possibly others) which run on fixed schedules (e.g. <code>dinstall</code> runs
at 01:52 UTC, 07:52 UTC, 13:52 UTC and 19:52 UTC).</p>

<p>Depending on timing, I estimated that you might wait for over 7 hours (!!)
before your package is actually installable.</p>

<p>What’s worse for me is that feedback to your upload is asynchronous. I like to
do one thing, be done with it, move to the next thing. The current setup
requires a many-minute wait and costly task switch for no good technical
reason. You might think a few minutes aren’t a big deal, but when all the time I
can spend on Debian per day is measured in minutes, this makes a huge difference
in perceived productivity and fun.</p>

<p>The last communication I can find about speeding up this process is <a href="https://lists.debian.org/debian-project/2008/12/msg00014.html">ganneff’s
post</a> from 2008.</p>

<p>How would things look like in a better world?</p>

<ol>
<li>Anonymous FTP would be replaced by a web service which ingests my package and
returns an authoritative accept or reject decision in its response.</li>
<li>For accepted packages, there would be a status page displaying the build
status and when the package will be available via the mirror network.</li>
<li>Packages should be available within a few minutes after the build completed.</li>
</ol>

<h4 id="old-infrastructure-bug-tracker">Old infrastructure: bug tracker</h4>

<p>I dread interacting with the Debian bug
tracker. <a href="https://en.wikipedia.org/wiki/Debbugs">debbugs</a> is a piece of software
(from 1994) which is only used by Debian and the GNU project these days.</p>

<p>Debbugs processes emails, which is to say it is asynchronous and cumbersome to
deal with. Despite running on the fastest machines we have available in Debian
(or so I was told when the subject last came up), its web interface loads very
slowly.</p>

<p>Notably, the web interface at bugs.debian.org is read-only. Setting up a working
email setup for
<a href="https://manpages.debian.org/stretch/reportbug/reportbug.1.en.html"><code>reportbug(1)</code></a>
or manually dealing with attachments is a rather big hurdle.</p>

<p>For reasons I don’t understand, every interaction with debbugs results in <a href="https://twitter.com/zekjur/status/1027995569770442752">many
different email <em>threads</em></a>.</p>

<p>Aside from the technical implementation, I also can never remember the different
ways that Debian uses pseudo-packages for bugs and processes. I need them rarely
enough to establish a mental model of how they are set up, or working memory of
how they are used, but frequently enough to be annoyed by this.</p>

<p>How would things look like in a better world?</p>

<ol>
<li>Debian would switch from a custom bug tracker to a (any) well-established
one.</li>
<li>Debian would offer automation around processes. It is great to have a
paper-trail and artifacts of the process in the form of a bug report, but the
primary interface should be more convenient (e.g. a web form).</li>
</ol>

<h4 id="old-infrastructure-mailing-list-archives">Old infrastructure: mailing list archives</h4>

<p>It baffles me that in 2019, we still don’t have a conveniently browsable
threaded archive of mailing list discussions. Email and threading is more widely
used in Debian than anywhere else, so this is somewhat
ironic. <a href="https://en.wikipedia.org/wiki/Gmane">Gmane</a> used to paper over this
issue, but Gmane’s availability over the last few years has been spotty, to say
the least (it is down as I write this).</p>

<p>I tried to contribute a threaded list archive, but our listmasters didn’t seem
to care or want to support the project.</p>

<h4 id="debian-is-hard-to-machine-read">Debian is hard to machine-read</h4>

<p>While it is obviously possible to deal with Debian packages programmatically,
the experience is far from pleasant. Everything seems slow and cumbersome. I
have picked just 3 quick examples to illustrate my point.</p>

<p><a href="https://github.com/Debian/debiman/">debiman</a> needs <a href="https://github.com/Debian/debiman/issues/12">help from
piuparts</a> in analyzing the
alternatives mechanism of each package to display the manpages of
e.g. <a href="https://manpages.debian.org/stretch/postgresql-client-9.6/psql.1.en.html"><code>psql(1)</code></a>. This
is because maintainer scripts modify the alternatives database by calling shell
scripts. Without actually installing a package, you cannot know which changes it
does to the alternatives database.</p>

<p><a href="https://github.com/Debian/pk4">pk4</a> needs to maintain its own cache to look up
package metadata based on the package name. Other tools parse the apt database
from scratch on every invocation. A proper database format, or at least a binary
interchange format, would go a long way.</p>

<p><a href="https://github.com/Debian/dcs/">Debian Code Search</a> wants to ingest new
packages as quickly as possible. There used to be a
<a href="https://github.com/fedora-infra/fedmsg">fedmsg</a> instance for Debian, but it no
longer seems to exist. It is unclear where to get notifications from for new
packages, and where best to fetch those packages.</p>

<h4 id="complicated-build-stack">Complicated build stack</h4>

<p>See my <a href="/posts/2016-11-25-build-tools/">“Debian package build tools”</a> post. It
really bugs me that the sprawl of tools is not seen as a problem by others.</p>

<h4 id="developer-experience-pretty-painful">Developer experience pretty painful</h4>

<p>Most of the points discussed so far deal with the experience in <em>developing
Debian</em>, but as I recently described in my post <a href="/posts/2019-02-15-debian-debugging-devex/">“Debugging experience in
Debian”</a>, the experience when
<em>developing using Debian</em> leaves a lot to be desired, too.</p>

<h4 id="i-have-more-ideas">I have more ideas</h4>

<p>At this point, the article is getting pretty long, and hopefully you got a rough
idea of my motivation.</p>

<p>While I described a number of specific shortcomings above, the final nail in the
coffin is actually the lack of a positive outlook. I have more ideas that seem
really compelling to me, but, based on how my previous projects have been going,
I don’t think I can make any of these ideas happen within the Debian project.</p>

<p>I intend to publish a few more posts about specific ideas for improving
operating systems here. Stay tuned.</p>

<p>Lastly, I hope this post inspires someone, ideally a group of people, to improve
the developer experience within Debian.</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Debugging experience in Debian]]></title>
    <link href="https://michael.stapelberg.ch/posts/2019-02-15-debian-debugging-devex/"/>
    <id>https://michael.stapelberg.ch/posts/2019-02-15-debian-debugging-devex/</id>
    <published>2019-02-15T00:00:00+00:00</published>
    <updated>2019-02-15T13:11:24+01:00</updated>
    <content type="html"><![CDATA[

<p>Recently, a user reported that <a href="https://bugs.debian.org/918925">they don’t see window titles in i3 when running
i3 on a Raspberry Pi with Debian</a>.</p>

<p>I copied the latest <a href="https://wiki.debian.org/RaspberryPi3">Raspberry Pi Debian
image</a> onto an SD card, booted it, and was
able to reproduce the issue.</p>

<p>Conceptually, at this point, I should be able to install and start <code>gdb</code>, set a
break point and step through the code.</p>

<h3 id="enabling-debug-symbols-in-debian">Enabling debug symbols in Debian</h3>

<p>Debian, by default, strips debug symbols when building packages to conserve disk
space and network bandwidth. The motivation is very reasonable: most users will
never need the debug symbols.</p>

<p>Unfortunately, obtaining debug symbols when you do need them is unreasonably hard.</p>

<p>We begin by configuring an additional apt repository which contains
automatically generated debug packages:</p>

<pre><code>raspi# cat &gt;&gt;/etc/apt/sources.list.d/debug.list &lt;&lt;'EOT'
deb http://deb.debian.org/debian-debug buster-debug main contrib non-free
EOT
raspi# apt update
</code></pre>

<p>Notably, not all Debian packages have debug packages. As <a href="https://wiki.debian.org/DebugPackage">the DebugPackage
Debian Wiki page</a> explains,
<code>debhelper/9.20151219</code> started generating debug packages (ending in <code>-dbgsym</code>)
automatically. Packages which have not been updated might come with their own
debug packages (ending in <code>-dbg</code>) or might not preserve debug symbols at all!</p>

<p>Now that we <strong>can</strong> install debug packages, how do we know <strong>which ones</strong> we need?</p>

<h3 id="finding-debug-symbol-packages-in-debian">Finding debug symbol packages in Debian</h3>

<p>For debugging i3, we obviously need at least the <code>i3-dbgsym</code> package, but i3
uses a number of other libraries through whose code we may need to step.</p>

<p>The <code>debian-goodies</code> package ships a tool called
<a href="https://manpages.debian.org/testing/debian-goodies/find-dbgsym-packages.1.en.html">find-dbgsym-packages</a>
which prints the required packages to debug an executable, core dump or running
process:</p>

<pre><code>raspi# apt install debian-goodies
raspi# apt install $(find-dbgsym-packages $(which i3))
</code></pre>

<p>Now we should have symbol names and line number information available in
<code>gdb</code>. But for effectively stepping through the program, access to the source
code is required.</p>

<h3 id="obtaining-source-code-in-debian">Obtaining source code in Debian</h3>

<p>Naively, one would assume that <code>apt source</code> should be sufficient for obtaining
the source code of any Debian package. However, <code>apt source</code> defaults to the
package candidate version, not the version you have installed on your
system.</p>

<p>I have addressed this issue with the
<a href="https://manpages.debian.org/testing/pk4/pk4.1.en.html"><code>pk4</code></a> tool, which
defaults to the installed version.</p>

<p>Before we can extract any sources, we need to configure yet another apt
repository:</p>

<pre><code>raspi# cat &gt;&gt;/etc/apt/sources.list.d/source.list &lt;&lt;'EOT'
deb-src http://deb.debian.org/debian buster main contrib non-free
EOT
raspi# apt update
</code></pre>

<p>Regardless of whether you use <code>apt source</code> or <code>pk4</code>, one remaining problem is
the directory mismatch: the debug symbols contain a certain path, and that path
is typically not where you extracted your sources to. While debugging, you will
need to tell <code>gdb</code> about the location of the sources. This is tricky when you
debug a call across different source packages:</p>

<pre><code>(gdb) pwd
Working directory /usr/src/i3.
(gdb) list main
229     * the main loop. */
230     ev_unref(main_loop);
231   }
232 }
233
234 int main(int argc, char *argv[]) {
235  /* Keep a symbol pointing to the I3_VERSION string constant so that
236   * we have it in gdb backtraces. */
237  static const char *_i3_version __attribute__((used)) = I3_VERSION;
238  char *override_configpath = NULL;
(gdb) list xcb_connect
484	../../src/xcb_util.c: No such file or directory.
</code></pre>

<p>See <a href="https://sourceware.org/gdb/onlinedocs/gdb/Source-Path.html">Specifying Source
Directories</a> in the
gdb manual for the <code>dir</code> command which allows you to add multiple directories to
the source path. This is pretty tedious, though, and does not work for all
programs.</p>

<h3 id="positive-example-fedora">Positive example: Fedora</h3>

<p>While Fedora conceptually shares all the same steps, the experience on Fedora is
so much better: when you run <code>gdb /usr/bin/i3</code>, it will tell you what the next
step is:</p>

<pre><code># gdb /usr/bin/i3
[…]
Reading symbols from /usr/bin/i3...(no debugging symbols found)...done.
Missing separate debuginfos, use: dnf debuginfo-install i3-4.16-1.fc28.x86_64
</code></pre>

<p>Watch what happens when we run the suggested command:</p>

<pre><code># dnf debuginfo-install i3-4.16-1.fc28.x86_64
enabling updates-debuginfo repository
enabling fedora-debuginfo repository
[…]
Installed:
  i3-debuginfo.x86_64 4.16-1.fc28
  i3-debugsource.x86_64 4.16-1.fc28
Complete!
</code></pre>

<p>A single command understood our intent, enabled the required repositories and
installed the required packages, both for debug symbols and source code (stored
in e.g. <code>/usr/src/debug/i3-4.16-1.fc28.x86_64</code>). Unfortunately, <code>gdb</code> doesn’t
seem to locate the sources, which seems like a bug to me.</p>

<p>One downside of Fedora’s approach is that <code>gdb</code> will only print all required
dependencies once you actually run the program, so you may need to run multiple
<code>dnf</code> commands.</p>

<h3 id="in-an-ideal-world">In an ideal world</h3>

<p>Ideally, none of the manual steps described above would be necessary. It seems
absurd to me that so much knowledge is required to efficiently debug programs in
Debian. Case in point: I only learnt about <code>find-dbgsym-packages</code> a few days ago
when talking to one of its contributors.</p>

<p>Installing <code>gdb</code> should be all that a user needs to do. Debug symbols and
sources can be transparently provided through a lazy-loading FUSE file
system. If our build/packaging infrastructure assured predictable paths and
automated debug symbol extraction, we could have transparent, quick and reliable
debugging of all programs within Debian.</p>

<p>NixOS’s dwarffs is an implementation of this idea:
<a href="https://github.com/edolstra/dwarffs">https://github.com/edolstra/dwarffs</a></p>

<h3 id="conclusion">Conclusion</h3>

<p>While I agree with the removal of debug symbols as a general optimization, I
think every Linux distribution should strive to provide an entirely transparent
debugging experience: you should not even have to know that debug symbols are
not present by default. Debian really falls short in this regard.</p>

<p>Getting Debian to a fully transparent debugging experience requires a lot of
technical work and a lot of social convincing. In my experience,
programmatically working with the Debian archive and packages is tricky, and
ensuring that <em>all</em> packages in a Debian release have debug packages (let alone
predictable paths) seems entirely unachievable due to the fragmentation of
packaging infrastructure and holdouts blocking any progress.</p>

<p>My go-to example is <a href="https://sources.debian.org/src/rsync/3.1.3-5/debian/rules/">rsync’s
debian/rules</a>, which
intentionally (!) still has not adopted debhelper. It is not a surprise that
there are no debug symbols for <code>rsync</code> in Debian.</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[TurboPFor: an analysis]]></title>
    <link href="https://michael.stapelberg.ch/posts/2019-02-05-turbopfor-analysis/"/>
    <id>https://michael.stapelberg.ch/posts/2019-02-05-turbopfor-analysis/</id>
    <published>2019-02-05T09:00:00+01:00</published>
    <updated>2019-02-05T09:18:27+01:00</updated>
    <content type="html"><![CDATA[

<h3 id="motivation">Motivation</h3>

<p>I have recently been looking into speeding up Debian Code Search. As a quick
reminder, search engines answer queries by consulting an inverted index: a map
from term to documents containing that term (called a “posting list”). See <a href="https://codesearch.debian.net/research/bsc-thesis.pdf">the
Debian Code Search Bachelor
Thesis</a> (PDF) for a lot
more details.</p>

<p>Currently, Debian Code Search does not store positional information in its
index, i.e. the index can only reveal <em>that</em> a certain trigram is present in a
document, not <em>where</em> or <em>how often</em>.</p>

<p>From analyzing Debian Code Search queries, I knew that identifier queries (70%)
massively outnumber regular expression queries (30%). When processing identifier
queries, storing positional information in the index enables a significant
optimization: instead of identifying the possibly-matching documents and having
to read them all, we can determine matches from querying the index alone, no
document reads required.</p>

<p>This moves the bottleneck: having to read all possibly-matching documents
requires a lot of expensive random I/O, whereas having to decode long posting
lists requires a lot of cheap sequential I/O.</p>

<p>Of course, storing positions comes with a downside: the index is larger, and a
larger index takes more time to decode when querying.</p>

<p>Hence, I have been looking at various posting list compression/decoding
techniques, to figure out whether we could switch to a technique which would
retain (or improve upon!) current performance despite much longer posting lists
and produce a small enough index to fit on our current hardware.</p>

<h3 id="literature">Literature</h3>

<p>I started looking into this space because of Daniel Lemire’s <a href="https://lemire.me/blog/2017/09/27/stream-vbyte-breaking-new-speed-records-for-integer-compression/">Stream
VByte</a>
post. As usual, Daniel’s work is well presented, easily digestible and
accompanied by not just one, but multiple implementations.</p>

<p>I also looked for scientific papers to learn about the state of the art and
classes of different approaches in general. The best I could find is
<a href="https://dl.acm.org/citation.cfm?doid=2682862.2682870">Compression, SIMD, and Postings
Lists</a>. If you don’t have
access to the paper, I hear that
<a href="https://en.wikipedia.org/wiki/Sci-Hub">Sci-Hub</a> is helpful.</p>

<p>The paper is from 2014, and doesn’t include all algorithms. If you know of a
better paper, please let me know and I’ll include it here.</p>

<p>Eventually, I stumbled upon an algorithm/implementation called TurboPFor, which
the rest of the article tries to shine some light on.</p>

<h3 id="turbopfor">TurboPFor</h3>

<p>If you’re wondering: PFor stands for Patched Frame Of Reference and describes a
family of algorithms. The principle is explained e.g. in <a href="https://arxiv.org/pdf/1401.6399.pdf">SIMD Compression and
the Intersection of Sorted Integers (PDF)</a>.</p>

<p>The <a href="https://github.com/powturbo/TurboPFor">TurboPFor project’s README file</a>
claims that TurboPFor256 compresses with a rate of 5.04 bits per integer, and
can decode with 9400 MB/s on a single thread of an Intel i7-6700 CPU.</p>

<p>For Debian Code Search, we use unsigned integers of 32 bit (uint32), which
TurboPFor will compress into as few bits as required.</p>

<p>Dividing Debian Code Search’s file sizes by the total number of integers, I get
similar values, at least for the docid index section:</p>

<ul>
<li>5.49 bits per integer for the docid index section</li>
<li>11.09 bits per integer for the positions index section</li>
</ul>

<p>I can confirm the order of magnitude of the decoding speed, too. My benchmark
calls TurboPFor from Go via cgo, which introduces some overhead. To exclude disk
speed as a factor, data comes from the page cache. The benchmark sequentially
decodes all posting lists in the specified index, using as many threads as the
machine has cores¹:</p>

<ul>
<li>≈1400 MB/s on a  1.1 GiB docid index section</li>
<li>≈4126 MB/s on a 15.0 GiB position index section</li>
</ul>

<p>I think the numbers differ because the position index section contains larger
integers (requiring more bits). I repeated both benchmarks, capped to 1 GiB, and
decoding speeds still differed, so it is not just the size of the index.</p>

<p>Compared to Streaming VByte, a TurboPFor256 index comes in at just over half the
size, while still reaching 83% of Streaming VByte’s decoding speed. This seems
like a good trade-off for my use-case, so I decided to have a closer look at how
TurboPFor works.</p>

<p>① See <a href="https://github.com/stapelberg/goturbopfor/blob/d7954fb81e66080941891dccc27407d8496f65d9/cmd/gp4-verify/verify.go">cmd/gp4-verify/verify.go</a> run on an Intel i9-9900K.</p>

<h3 id="methodology">Methodology</h3>

<p>To confirm my understanding of the details of the format, I implemented a
pure-Go TurboPFor256 decoder. Note that it is intentionally <em>not optimized</em> as
its main goal is to use simple code to teach the TurboPFor256 on-disk format.</p>

<p>If you’re looking to use TurboPFor from Go, I recommend using cgo. cgo’s
function call overhead is about 51ns <a href="https://go-review.googlesource.com/c/go/+/30080">as of Go
1.8</a>, which will easily be
offset by TurboPFor’s carefully optimized, vectorized (SSE/AVX) code.</p>

<p>With that caveat out of the way, you can find my teaching implementation at
<a href="https://github.com/stapelberg/goturbopfor">https://github.com/stapelberg/goturbopfor</a></p>

<p>I verified that it produces the same results as TurboPFor’s <code>p4ndec256v32</code>
function for all posting lists in the Debian Code Search index.</p>

<h3 id="on-disk-format">On-disk format</h3>

<p>Note that TurboPFor does not fully define an on-disk format on its own. When
encoding, it turns a list of integers into a byte stream:</p>

<pre><code>size_t p4nenc256v32(uint32_t *in, size_t n, unsigned char *out);
</code></pre>

<p>When decoding, it decodes the byte stream into an array of integers, but needs
to know the number of integers in advance:</p>

<pre><code>size_t p4ndec256v32(unsigned char *in, size_t n, uint32_t *out);
</code></pre>

<p>Hence, you’ll need to keep track of the number of integers and length of the
generated byte streams separately. When I talk about on-disk format, I’m
referring to the byte stream which TurboPFor returns.</p>

<p>The TurboPFor256 format uses blocks of 256 integers each, followed by a trailing
block — if required — which can contain fewer than 256 integers:</p>

<p><img src="/turbopfor/ondisk.svgo.svg"></p>

<p>SIMD bitpacking is used for all blocks but the trailing block (which uses
regular bitpacking). This is not merely an implementation detail for decoding:
the on-disk structure is different for blocks which can be SIMD-decoded.</p>

<p>Each block starts with a 2 bit header, specifying the type of the block:</p>

<ul>
<li>11: <a href="#block-constant">constant</a></li>
<li>00: <a href="#block-bitpack">bitpacking</a></li>
<li>10: <a href="#block-bitpackex">bitpacking with exceptions (bitmap)</a></li>
<li>01: <a href="#block-bitpackvb">bitpacking with exceptions (variable byte)</a></li>
</ul>

<p>Each block type is explained in more detail in the following sections.</p>

<p>Note that none of the block types store the number of elements: you will always
need to know how many integers you need to decode. Also, you need to know in
advance how many bytes you need to feed to TurboPFor, so you will need some sort
of container format.</p>

<p>Further, TurboPFor automatically choses the best block type for each block.</p>

<h4 id="block-constant">Constant block</h4>

<p>A constant block (all integers of the block have the same value) consists of a
single value of a specified bit width ≤ 32. This value will be stored in each
output element for the block. E.g., after calling <code>decode(input, 3, output)</code>
with <code>input</code> being the constant block depicted below, output is <code>{0xB8912636,
0xB8912636, 0xB8912636}</code>.</p>

<p><img src="/turbopfor/block-constant.svgo.svg"></p>

<p>The example shows the maximum number of bytes (5). Smaller integers will use
fewer bytes: e.g. an integer which can be represented in 3 bits will only use 2
bytes.</p>

<h4 id="block-bitpack">Bitpacking block</h4>

<p>A bitpacking block specifies a bit width ≤ 32, followed by a stream of
bits. Each value starts at the Least Significant Bit (LSB), i.e. the 3-bit
values 0 (<code>000b</code>) and 5 (<code>101b</code>) are encoded as <code>101000b</code>.</p>

<p><img src="/turbopfor/block-bitpack.svgo.svg"></p>

<h4 id="block-bitpackex">Bitpacking with exceptions (bitmap) block</h4>

<p>The constant and bitpacking block types work well for integers which don’t
exceed a certain width, e.g. for a series of integers of width ≤ 5 bits.</p>

<p>For a series of integers where only a few values exceed an otherwise common
width (say, two values require 7 bits, the rest requires 5 bits), it makes sense
to cut the integers into two parts: value and exception.</p>

<p>In the example below, decoding the third integer <code>out2</code> (<code>000b</code>) requires
combination with exception <code>ex0</code> (<code>10110b</code>), resulting in <code>10110000b</code>.</p>

<p>The number of exceptions can be determined by summing the 1 bits in the bitmap
using the <a href="https://en.wikipedia.org/wiki/Hamming_weight">popcount instruction</a>.</p>

<p><img src="/turbopfor/block-bitpackex.svgo.svg"></p>

<h4 id="block-bitpackvb">Bitpacking with exceptions (variable byte)</h4>

<p>When the exceptions are not uniform enough, it makes sense to switch from
bitpacking to a variable byte encoding:</p>

<p><img src="/turbopfor/block-bitpackvb.svgo.svg"></p>

<h3 id="decoding-variable-byte">Decoding: variable byte</h3>

<p>The variable byte encoding used by the TurboPFor format is similar to the one
<a href="https://sqlite.org/src4/doc/trunk/www/varint.wiki">used by SQLite</a>, which is
described, alongside other common variable byte encodings, at
<a href="https://github.com/stoklund/varint">github.com/stoklund/varint</a>.</p>

<p>Instead of using individual bits for dispatching, this format classifies the
first byte (<code>b[0]</code>) into ranges:</p>

<ul>
<li>[0—176]: the value is <code>b[0]</code></li>
<li>[177—240]: a 14 bit value is in <code>b[0]</code> (6 high bits) and <code>b[1]</code> (8 low bits)</li>
<li>[241—248]: a 19 bit value is in <code>b[0]</code> (3 high bits), <code>b[1]</code> and <code>b[2]</code> (16 low bits)</li>
<li>[249—255]: a 32 bit value is in <code>b[1]</code>, <code>b[2]</code>, <code>b[3]</code> and possibly <code>b[4]</code></li>
</ul>

<p>Here is the space usage of different values:</p>

<ul>
<li>[0—176] are stored in 1 byte (as-is)</li>
<li>[177—16560] are stored in 2 bytes, with the highest 6 bits added to 177</li>
<li>[16561—540848] are stored in 3 bytes, with the highest 3 bits added to 241</li>
<li>[540849—16777215] are stored in 4 bytes, with 0 added to 249</li>
<li>[16777216—4294967295] are stored in 5 bytes, with 1 added to 249</li>
</ul>

<p>An overflow marker will be used to signal that encoding the
values would be less space-efficient than simply copying them
(e.g. if all values require 5 bytes).</p>

<p>This format is very space-efficient: it packs 0-176 into a single byte, as
opposed to 0-128 (most others). At the same time, it can be decoded very
quickly, as only the first byte needs to be compared to decode a value (similar
to PrefixVarint).</p>

<h3 id="decoding-bitpacking">Decoding: bitpacking</h3>

<h4 id="regular-bitpacking">Regular bitpacking</h4>

<p>In regular (non-SIMD) bitpacking, integers are stored on disk one after the
other, padded to a full byte, as a byte is the smallest addressable unit when
reading data from disk. For example, if you bitpack only one 3 bit int, you will
end up with 5 bits of padding.</p>

<p><img src="/turbopfor/bitpacking.svgo.svg"></p>

<h4 id="simd-bitpacking-256v32">SIMD bitpacking (256v32)</h4>

<p>SIMD bitpacking works like regular bitpacking, but processes 8 uint32
little-endian values at the same time, leveraging the <a href="https://en.wikipedia.org/wiki/Advanced_Vector_Extensions">AVX instruction
set</a>. The following
illustration shows the order in which 3-bit integers are decoded from disk:</p>

<p><img src="/turbopfor/bitpacking256v32.svgo.svg"></p>

<h3 id="in-practice">In Practice</h3>

<p>For a Debian Code Search index, 85% of posting lists are short enough to only
consist of a trailing block, i.e. no SIMD instructions can be used for decoding.</p>

<p>The distribution of block types looks as follows:</p>

<ul>
<li>72% bitpacking with exceptions (bitmap)</li>
<li>19% bitpacking with exceptions (variable byte)</li>
<li>5% constant</li>
<li>4% bitpacking</li>
</ul>

<p>Constant blocks are mostly used for posting lists with just one entry.</p>

<h3 id="conclusion">Conclusion</h3>

<p>The TurboPFor on-disk format is very flexible: with its 4 different kinds of
blocks, chances are high that a very efficient encoding will be used for most
integer series.</p>

<p>Of course, the flip side of covering so many cases is complexity: the format and
implementation take quite a bit of time to understand — hopefully this article
helps a little! For environments where the C TurboPFor implementation cannot be
used, smaller algorithms might be simpler to implement.</p>

<p>That said, if you can use the TurboPFor implementation, you will benefit from a
highly optimized SIMD code base, which will most likely be an improvement over
what you’re currently using.</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Looking for a new Raspberry Pi image maintainer]]></title>
    <link href="https://michael.stapelberg.ch/posts/2018-06-03-raspi3-looking-for-maintainer/"/>
    <id>https://michael.stapelberg.ch/posts/2018-06-03-raspi3-looking-for-maintainer/</id>
    <published>2018-06-03T08:43:00+02:00</published>
    <updated>2019-02-05T09:42:48+01:00</updated>
    <content type="html"><![CDATA[<p>
  <strong>This is taken care of: Gunnar Wolf has taken on maintenance of the Raspberry Pi image. Thank you!</strong>
</p>

<p>
  (Cross-posting this message I sent to pkg-raspi-maintainers for broader visibility.)
</p>

<p>
  I started building Raspberry Pi images because I thought there should be an easy, official way to install Debian on the Raspberry Pi.
</p>

<p>
  I still believe that, but I’m not actually using Debian on any of my Raspberry Pis anymore¹, so my personal motivation to do any work on the images is gone.
</p>

<p>
  On top of that, I realize that my commitments exceed my spare time capacity, so I need to get rid of responsibilities.
</p>

<p>
  Therefore, <strong>I’m looking for someone to take up maintainership of the Raspberry Pi images</strong>. Numerous people have reached out to me with thank you notes and questions, so I think the user interest is there. Also, I’ll be happy to answer any questions that you might have and that I can easily answer. Please reply here (or in private) if you’re interested.
</p>

<p>
  If I can’t find someone within the next 7 days, I’ll put up an announcement message in the raspi3-image-spec README, wiki page, and my blog posts, stating that the image is unmaintained and looking for a new maintainer.
</p>

<p>
  Thanks for your understanding,
</p>

<p>
  ① just in case you’re curious, I’m now running cross-compiled Go programs directly under a Linux kernel and minimal userland, see <a href="https://gokrazy.org/">https://gokrazy.org/</a>
</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[sbuild-debian-developer-setup(1)]]></title>
    <link href="https://michael.stapelberg.ch/posts/2018-03-19-sbuild-debian-developer-setup/"/>
    <id>https://michael.stapelberg.ch/posts/2018-03-19-sbuild-debian-developer-setup/</id>
    <published>2018-03-19T08:00:00+01:00</published>
    <updated>2019-02-04T19:11:20+01:00</updated>
    <content type="html"><![CDATA[<p>
  I have heard a number of times that sbuild is too hard to get started with,
  and hence people don’t use it.
</p>

<p>
  To reduce hurdles from using/contributing to Debian, I wanted to make sbuild
  easier to set up.
</p>

<p>
  sbuild ≥ 0.74.0 provides a Debian package
  called <a href="https://packages.debian.org/sid/sbuild-debian-developer-setup">sbuild-debian-developer-setup</a>. Once
  installed, run
  the <a href="https://manpages.debian.org/unstable/sbuild/sbuild-debian-developer-setup.1">sbuild-debian-developer-setup(1)</a>
  command to create a chroot suitable for building packages for Debian unstable.
</p>

<p>
  On a system without any sbuild/schroot bits installed, a transcript of the
  full setup looks like this:
</p>

<pre>
% sudo apt install -t unstable sbuild-debian-developer-setup
Reading package lists... Done
Building dependency tree
Reading state information... Done
The following additional packages will be installed:
  libsbuild-perl sbuild schroot
Suggested packages:
  deborphan btrfs-tools aufs-tools | unionfs-fuse qemu-user-static
Recommended packages:
  exim4 | mail-transport-agent autopkgtest
The following NEW packages will be installed:
  libsbuild-perl sbuild sbuild-debian-developer-setup schroot
0 upgraded, 4 newly installed, 0 to remove and 1454 not upgraded.
Need to get 1.106 kB of archives.
After this operation, 3.556 kB of additional disk space will be used.
Do you want to continue? [Y/n]
Get:1 http://localhost:3142/deb.debian.org/debian unstable/main amd64 libsbuild-perl all 0.74.0-1 [129 kB]
Get:2 http://localhost:3142/deb.debian.org/debian unstable/main amd64 sbuild all 0.74.0-1 [142 kB]
Get:3 http://localhost:3142/deb.debian.org/debian testing/main amd64 schroot amd64 1.6.10-4 [772 kB]
Get:4 http://localhost:3142/deb.debian.org/debian unstable/main amd64 sbuild-debian-developer-setup all 0.74.0-1 [62,6 kB]
Fetched 1.106 kB in 0s (5.036 kB/s)
Selecting previously unselected package libsbuild-perl.
(Reading database ... 276684 files and directories currently installed.)
Preparing to unpack .../libsbuild-perl_0.74.0-1_all.deb ...
Unpacking libsbuild-perl (0.74.0-1) ...
Selecting previously unselected package sbuild.
Preparing to unpack .../sbuild_0.74.0-1_all.deb ...
Unpacking sbuild (0.74.0-1) ...
Selecting previously unselected package schroot.
Preparing to unpack .../schroot_1.6.10-4_amd64.deb ...
Unpacking schroot (1.6.10-4) ...
Selecting previously unselected package sbuild-debian-developer-setup.
Preparing to unpack .../sbuild-debian-developer-setup_0.74.0-1_all.deb ...
Unpacking sbuild-debian-developer-setup (0.74.0-1) ...
Processing triggers for systemd (236-1) ...
Setting up schroot (1.6.10-4) ...
Created symlink /etc/systemd/system/multi-user.target.wants/schroot.service → /lib/systemd/system/schroot.service.
Setting up libsbuild-perl (0.74.0-1) ...
Processing triggers for man-db (2.7.6.1-2) ...
Setting up sbuild (0.74.0-1) ...
Setting up sbuild-debian-developer-setup (0.74.0-1) ...
Processing triggers for systemd (236-1) ...

% sudo sbuild-debian-developer-setup
The user `michael' is already a member of `sbuild'.
I: SUITE: unstable
I: TARGET: /srv/chroot/unstable-amd64-sbuild
I: MIRROR: http://localhost:3142/deb.debian.org/debian
I: Running debootstrap --arch=amd64 --variant=buildd --verbose --include=fakeroot,build-essential,eatmydata --components=main --resolve-deps unstable /srv/chroot/unstable-amd64-sbuild http://localhost:3142/deb.debian.org/debian
I: Retrieving InRelease 
I: Checking Release signature
I: Valid Release signature (key id 126C0D24BD8A2942CC7DF8AC7638D0442B90D010)
I: Retrieving Packages 
I: Validating Packages 
I: Found packages in base already in required: apt 
I: Resolving dependencies of required packages...
[…]
I: Successfully set up unstable chroot.
I: Run "sbuild-adduser" to add new sbuild users.
ln -s /usr/share/doc/sbuild/examples/sbuild-update-all /etc/cron.daily/sbuild-debian-developer-setup-update-all
Now run `newgrp sbuild', or log out and log in again.

% newgrp sbuild

% sbuild -d unstable hello
sbuild (Debian sbuild) 0.74.0 (14 Mar 2018) on x1

+==============================================================================+
| hello (amd64)                                Mon, 19 Mar 2018 07:46:14 +0000 |
+==============================================================================+

Package: hello
Distribution: unstable
Machine Architecture: amd64
Host Architecture: amd64
Build Architecture: amd64
Build Type: binary
[…]
</pre>

<p>
  I hope you’ll find this useful.
</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[dput usability changes]]></title>
    <link href="https://michael.stapelberg.ch/posts/2018-03-10-dput-usability/"/>
    <id>https://michael.stapelberg.ch/posts/2018-03-10-dput-usability/</id>
    <published>2018-03-10T10:00:00+01:00</published>
    <updated>2019-02-04T19:11:20+01:00</updated>
    <content type="html"><![CDATA[<p>
  dput-ng ≥ 1.16 contains two usability changes which make uploading easier:
</p>

<ol>
  <li>
    When no arguments are specified, dput-ng auto-selects the most recent .changes file (with confirmation).
  </li>
  <li>
    Instead of erroring out when detecting an unsigned .changes file, <a href="https://manpages.debian.org/stretch/devscripts/debsign.1">debsign(1)</a> is invoked to sign the .changes file before proceeding.
  </li>
</ol>

<p>
  With these changes, after building a package, you just need to
  type <code>dput</code> (in the correct directory of course) to sign and upload
  it.
</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[pristine-tar considered harmful]]></title>
    <link href="https://michael.stapelberg.ch/posts/2018-01-28-pristine-tar/"/>
    <id>https://michael.stapelberg.ch/posts/2018-01-28-pristine-tar/</id>
    <published>2018-01-28T21:20:00+01:00</published>
    <updated>2019-02-04T19:11:20+01:00</updated>
    <content type="html"><![CDATA[<p>
  If you want to follow along at home, clone this repository:
</p>

<pre>
% GBP_CONF_FILES=:debian/gbp.conf gbp clone https://anonscm.debian.org/git/pkg-go/packages/golang-github-go-macaron-inject.git
</pre>

<p>
  Now, in the <code>golang-github-go-macaron-inject</code> directory, I’m aware
  of three ways to obtain an orig tarball (please correct me if there are more):
</p>

<ol>
  <li>
    Run <code><a href="https://manpages.debian.org/stretch/git-buildpackage/gbp-buildpackage.1.en.html">gbp
    buildpackage</a></code>, creating an orig tarball from git
    (<code>upstream/0.0_git20160627.0.d8a0b86</code>)<br>
    The resulting sha1sum is <code>d085a04b7b35856be24f8cc4a9a6d9799cdb59b4</code>.
  </li>
  <li>
    Run <code><a href="https://manpages.debian.org/stretch/pristine-tar/pristine-tar.1.en.html">pristine-tar</a>
    checkout</code><br>
    The resulting sha1sum is <code>d51575c0b00db5fe2bbf8eea65bc7c4f767ee954</code>.
  </li>
  <li>
    Run <code><a href="https://manpages.debian.org/stretch/devscripts/origtargz.1.en.html">origtargz</a></code><br>
    The resulting sha1sum is <code>d51575c0b00db5fe2bbf8eea65bc7c4f767ee954</code>.
  </li>
</ol>

<p>
  Have a look at the
  archive’s <a href="https://deb.debian.org/debian/pool/main/g/golang-github-go-macaron-inject/golang-github-go-macaron-inject_0.0~git20160627.0.d8a0b86-2.dsc">golang-github-go-macaron-inject_0.0~git20160627.0.d8a0b86-2.dsc</a>,
  however: the file entry orig tarball reads:
</p>

<pre>
f5d5941c7b77e8941498910b64542f3db6daa3c2 7688 golang-github-go-macaron-inject_0.0~git20160627.0.d8a0b86.orig.tar.xz
</pre>

<p>
  So, why did we get a different tarball? Let’s go through the methods:
</p>

<ol>
  <li>
    The uploader must not have used <code>gbp buildpackage</code> to create
    their tarball. Perhaps they imported from a tarball created by
    dh-make-golang, or created manually, and then left that tarball in place
    (which is a perfectly fine, normal workflow).
  </li>

  <li>
    I’m not entirely sure why <code>pristine-tar</code> resulted in a different
    tarball than what’s in the archive. I think the most likely theory is that
    the uploader had to go back and modify the tarball, but forgot to update (or
    made a mistake while updating) the pristine-tar branch.
  </li>
  
  <li>
    <code>origtargz</code>, when it detects pristine-tar data, uses
    pristine-tar, hence the same tarball as ②.
  </li>
</ol>

<p>
  Had we not used pristine-tar for this repository at
  all, <code>origtargz</code> would have pulled the correct tarball from the
  archive.
</p>

<p>
  The above anecdote illustrates the fragility of the pristine-tar approach. In
  my experience from the pkg-go team, when the pristine-tar branch doesn’t
  contain outright incorrect data, it is often outdated. Even when everything is
  working correctly, a number of packagers are disgruntled about the extra
  work/mental complexity.
</p>

<p>
  In the pkg-go team, we have (independently of this specific anecdote)
  collectively decided to have the upstream branch track the upstream remote’s
  master (or similar) branch directly, and get rid of pristine-tar in our
  repositories. This should result in method ① and ③ working correctly.
</p>

<p>
  In conclusion, my recommendation for any repository is: don’t bother with
  pristine-tar. Instead, configure <code>origtargz</code> as a git-buildpackage
  postclone hook in your <code>~/.gbp.conf</code> to always work with archive
  orig tarballs:
</p>

<pre>
[clone]
# Ensure the correct orig tarball is present.
postclone=origtargz

[buildpackage]
# Pick up the orig tarballs created by the origtargz postclone hook.
tarball-dir = ..
</pre>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Debian buster on the Raspberry Pi 3 (update)]]></title>
    <link href="https://michael.stapelberg.ch/posts/2018-01-08-raspberry-pi-3/"/>
    <id>https://michael.stapelberg.ch/posts/2018-01-08-raspberry-pi-3/</id>
    <published>2018-01-08T22:55:00+01:00</published>
    <updated>2019-02-04T19:11:20+01:00</updated>
    <content type="html"><![CDATA[<p>
I previously wrote about <a
href="https://people.debian.org/~stapelberg/2017/10/08/raspberry-pi-3.html">my
Debian buster preview image for the Raspberry Pi 3</a>.
</p>

<p>
Now, I’m publishing an updated version, containing the following changes:
</p>
<ul>
<li>
WiFi works out of the box. Use e.g. <code>ip link set dev wlan0 up</code>, and <code>iwlist wlan0 scan</code>.
</li>
<li>
Kernel boot messages are now displayed on an attached monitor (if any), not just on the serial console.
</li>
<li>
Root file system resizing will now not touch the partition table if the user modified it.
</li>
<li>
The image is now compressed using xz, reducing its size to 170M.
</li>
</ul>

<p>
As before, the image is built
with <a href="https://github.com/larswirzenius/vmdb2">vmdb2</a>, the successor
to vmdebootstrap. The input files are available
at <a href="https://github.com/Debian/raspi3-image-spec">https://github.com/Debian/raspi3-image-spec</a>.
</p>

<p>
Note that Bluetooth is still untested
(see <a href="https://wiki.debian.org/RaspberryPi3">wiki:RaspberryPi3</a> for
details).
</p>

<p>
Given that Bluetooth is the only known issue, I’d like to work towards getting
this image built and provided on official Debian infrastructure. If you know how
to make this happen, please send me an email. Thanks!
</p>

<p>
As a <strong>preview version</strong> (i.e. unofficial, unsupported, etc.)
until that’s done, I built and uploaded the resulting image. Find it at <a
href="https://people.debian.org/~stapelberg/raspberrypi3/2018-01-08/">https://people.debian.org/~stapelberg/raspberrypi3/2018-01-08/</a>.
To install the image, insert the SD card into your computer (I’m assuming it’s
available as <code>/dev/sdb</code>) and copy the image onto it:
</p>

<pre>
$ wget https://people.debian.org/~stapelberg/raspberrypi3/2018-01-08/2018-01-08-raspberry-pi-3-buster-PREVIEW.img.xz
$ xzcat 2018-01-08-raspberry-pi-3-buster-PREVIEW.img.xz | dd of=/dev/sdb bs=64k oflag=dsync status=progress
</pre>

<p>
If resolving client-supplied DHCP hostnames works in your network, you should
be able to log into the Raspberry Pi 3 using SSH after booting it:
</p>

<pre>
$ ssh root@rpi3
# Password is “raspberry”
</pre>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Which VCS do Debian’s Go package upstreams use?]]></title>
    <link href="https://michael.stapelberg.ch/posts/2017-10-22-pkg-go-upstreams/"/>
    <id>https://michael.stapelberg.ch/posts/2017-10-22-pkg-go-upstreams/</id>
    <published>2017-10-22T13:20:00+02:00</published>
    <updated>2019-02-04T19:11:20+01:00</updated>
    <content type="html"><![CDATA[<p>
  In the pkg-go team, we are currently discussing which workflows we should
  standardize on.
</p>

<p>
  One of the considerations is what goes into the “upstream” Git branch of our
  repositories: should it track the upstream Git repository, or should it
  contain orig tarball imports?
</p>

<p>
  Now, tracking the upstream Git repository only works if upstream actually uses
  Git. The go tool, which is widely used within the Go community for managing Go
  packages, supports Git, Mercurial, Bazaar and Subversion. But which of these
  are actually used in practice?
</p>

<p>
  Let’s find out!
</p>

<h3>Option 1: If you have the sources lists of all suites locally anyway</h3>

<pre>
/usr/lib/apt/apt-helper cat-file \
  $(apt-get indextargets --format '$(FILENAME)' 'ShortDesc: Sources' 'Origin: Debian') \
  | sed -n 's,Go-Import-Path: ,,gp' \
  | sort -u
</pre>

<h3>Option 2: If you prefer to use a relational database over textfiles</h3>

<p>
  This is the harder option, but also the more complete one.
</p>

<p>
  First, we’ll need the Go package import paths of all Go packages which are in
  Debian. We can get them from
  the <a href="https://wiki.debian.org/ProjectB">ProjectB</a> database, Debian’s
  main PostgreSQL database containing all of the state about the Debian archive.
</p>

<p>
  Unfortunately, only Debian Developers have SSH access to a mirror of ProjectB
  at the moment. I contacted DSA to ask about providing public ProjectB access.
</p>

<pre>
  ssh mirror.ftp-master.debian.org "echo \"SELECT value FROM source_metadata \
  LEFT JOIN metadata_keys ON (source_metadata.key_id = metadata_keys.key_id) \
  WHERE metadata_keys.key = 'Go-Import-Path' GROUP BY value\" | \
    psql -A -t service=projectb" > go_import_path.txt
</pre>

<p>
  I
  uploaded <a href="https://people.debian.org/~stapelberg/2017-10-22-go_import_path.txt">a
  copy of resulting <code>go_import_path.txt</code></a>, if you’re curious.
</p>

<p>
  Now, let’s come up with a little bit of Go to print the VCS responsible for
  each specified Go import path:
</p>

<pre>
go get -u golang.org/x/tools/go/vcs
cat >vcs4.go <<'EOT'
package main

import (
	"fmt"
	"log"
	"os"
	"sync"

	"golang.org/x/tools/go/vcs"
)

func main() {
	var wg sync.WaitGroup
	for _, arg := range os.Args[1:] {
		wg.Add(1)
		go func(arg string) {
			defer wg.Done()
			rr, err := vcs.RepoRootForImportPath(arg, false)
			if err != nil {
				log.Println(err)
				return
			}
			fmt.Println(rr.VCS.Name)
		}(arg)
	}
	wg.Wait()
}
EOT
</pre>

<p>
  Lastly, run it in combination
  with <a href="https://manpages.debian.org/stretch/coreutils/uniq.1"><code>uniq(1)</code></a>
  to discover…
</p>

<pre>
go run vcs4.go $(tr '\n' ' ' < go_import_path.txt) | sort | uniq -c
    760 Git
      1 Mercurial
</pre>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[pk4: a new tool to avail the Debian source package producing the specified package]]></title>
    <link href="https://michael.stapelberg.ch/posts/2017-10-21-pk4/"/>
    <id>https://michael.stapelberg.ch/posts/2017-10-21-pk4/</id>
    <published>2017-10-21T10:05:00+02:00</published>
    <updated>2019-02-04T19:11:20+01:00</updated>
    <content type="html"><![CDATA[<p>
  UNIX distributions used to come with the system source code
  in <code>/usr/src</code>. This is a concept which fascinates me: if you want
  to change something in any part of your system, just make your change in the
  corresponding directory, recomile, reinstall, and you can immediately see your
  changes in action.
</p>

<p>
  So, I decided I wanted to build a tool which can give you the impression of
  that, without the downsides of additional disk space usage and slower update
  times because of <code>/usr/src</code> maintenance.
</p>

<p>
  The result of this effort is a tool called <code>pk4</code> (mnemonic: get me
  the package for…) which I just uploaded to Debian.
</p>

<p>
  What distinguishes this tool from an <code>apt source</code> call is the
  combination of a number of features:
</p>

<ul>
  <li>
    pk4 defaults to the version of the package which is installed on your
    system. This means when installing the resulting packages, you won’t be
    forced to upgrade your system in case you’re not running the latest
    available version.
    <br>
    In case the package is not installed on your system, the candidate
    (see <code>apt policy</code>) will be used.
  </li>
  <li>
    pk4 tries hard to resolve the provided argument(s): you can specify Debian
    binary package names, Debian source package names, or file paths on your
    system (in which case the owning package will be used).
  </li>
  <li>
    pk4 comes with tab completion for bash and zsh.
  </li>
  <li>
    pk4 caps the disk usage of the checked out packages by deleting the oldest ones
    after crossing a limit (default: 2GiB).
  </li>
  <li>
    pk4 allows users to enable supplied or shipped-with-pk4 hooks, e.g. git-init.
    <br>
    The git-init hook in particular results in an experience that reminds of
    <a href="https://manpages.debian.org/stretch/dgit/dgit.1.en.html">dgit</a>,
    and in fact it might be useful to combine the two tools in some way.
  </li>
  <li>
    pk4 optimizes for low latency of each operation.
  </li>
  <li>
    pk4 respects your APT configuration, i.e. should work in company intranets.
  </li>
  <li>
    tries hard to download source packages, with fallback to snapshot.debian.org.
  </li>
</ul>

<p>
  If you don’t want to wait for the package to clear the NEW queue, you can get
  it from here in the meantime:
</p>
<pre>
wget https://people.debian.org/~stapelberg/pk4/pk4_1_amd64.deb
sudo apt install ./pk4_1_amd64.deb
</pre>

<p>
  You can find the sources and issue tracker
  at <a href="https://github.com/Debian/pk4">https://github.com/Debian/pk4</a>.
</p>

<p>
  Here is a terminal screencast of the tool in action, availing the sources of
  i3, applying a patch, rebuilding the package and replacing the installed
  binary packages:
</p>

<script type="text/javascript" src="https://asciinema.org/a/TgYn2wkABiob14WKb2UefNl9f.js" id="asciicast-TgYn2wkABiob14WKb2UefNl9f" async></script>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Debian stretch on the Raspberry Pi 3 (update)]]></title>
    <link href="https://michael.stapelberg.ch/posts/2017-10-08-raspberry-pi-3/"/>
    <id>https://michael.stapelberg.ch/posts/2017-10-08-raspberry-pi-3/</id>
    <published>2017-10-08T22:45:00+02:00</published>
    <updated>2019-02-04T19:11:20+01:00</updated>
    <content type="html"><![CDATA[<p>
I previously wrote about <a
href="https://people.debian.org/~stapelberg/2017/03/22/raspberry-pi-3.html">my
Debian stretch preview image for the Raspberry Pi 3</a>.
</p>

<p>
Now, I’m publishing an updated version, containing the following changes:
</p>
<ul>
<li>
SSH host keys are generated on first boot.
</li>
<li>
Old kernel versions are now removed from /boot/firmware when purged.
</li>
<li>
The image is built
with <a href="https://github.com/larswirzenius/vmdb2">vmdb2</a>, the successor
to vmdebootstrap. The input files are available
at <a href="https://github.com/Debian/raspi3-image-spec">https://github.com/Debian/raspi3-image-spec</a>.
</li>
<li>
The image uses the linux-image-arm64 4.13.4-3 kernel, which provides HDMI output.
</li>
<li>
The image is now compressed using bzip2, reducing its size to 220M.
</li>
</ul>

<p>
A couple of issues remain, notably the lack of WiFi and bluetooth support
(see <a
href="https://wiki.debian.org/RaspberryPi3">wiki:RaspberryPi3</a> for details.
Any help with fixing these issues is very welcome!
</p>

<p>
As a <strong>preview version</strong> (i.e. unofficial, unsupported, etc.)
until all the necessary bits and pieces are in place to build images in a
proper place in Debian, I built and uploaded the resulting image. Find it at <a
href="https://people.debian.org/~stapelberg/raspberrypi3/2017-10-08/">https://people.debian.org/~stapelberg/raspberrypi3/2017-10-08/</a>.
To install the image, insert the SD card into your computer (I’m assuming it’s
available as <code>/dev/sdb</code>) and copy the image onto it:
</p>

<pre>
$ wget https://people.debian.org/~stapelberg/raspberrypi3/2017-10-08/2017-10-08-raspberry-pi-3-buster-PREVIEW.img.bz2
$ bunzip2 2017-10-08-raspberry-pi-3-buster-PREVIEW.img.bz2
$ sudo dd if=2017-10-08-raspberry-pi-3-buster-PREVIEW.img of=/dev/sdb bs=5M
</pre>

<p>
If resolving client-supplied DHCP hostnames works in your network, you should
be able to log into the Raspberry Pi 3 using SSH after booting it:
</p>

<pre>
$ ssh root@rpi3
# Password is “raspberry”
</pre>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[manpages.debian.org: what’s new since the launch?]]></title>
    <link href="https://michael.stapelberg.ch/posts/2017-04-09-manpages-debian-org-news/"/>
    <id>https://michael.stapelberg.ch/posts/2017-04-09-manpages-debian-org-news/</id>
    <published>2017-04-09T13:23:00+02:00</published>
    <updated>2019-02-04T19:11:20+01:00</updated>
    <content type="html"><![CDATA[<p>
On 2017-01-18, I announced that <a
href="https://manpages.debian.org">https://manpages.debian.org</a> had been
modernized. Let me catch you up on a few things which happened in the meantime:
</p>

<ul>
<li>
Debian experimental was added to manpages.debian.org. I was <a
href="https://github.com/Debian/debiman/issues/23">surprised to learn</a> that
adding experimental only required 52MB of disk usage. Further, Debian contrib
was added after <a href="https://github.com/Debian/debiman/issues/34">realizing
that contrib licenses are compatible with the DFSG</a>.
</li>
<li>
Indentation in some code examples was <a
href="https://github.com/Debian/debiman/issues/21">fixed upstream</a> in
<a href="http://mdocml.bsd.lv">mandoc</a>.
</li>
<li>
Address-bar search should now also work in Firefox, which <a
href="https://github.com/Debian/debiman/issues/41">apparently requires a title
attribute</a> on the opensearch XML file reference.
</li>
<li>
manpages now <a href="https://github.com/Debian/debiman/issues/42">specify
their language</a> in the HTML tag so that search engines can offer users the
most appropriate version of the manpage.
</li>
<li>
I contributed <code>mandocd(8)</code> to the mandoc project, which <a
href="https://github.com/Debian/debiman/commit/3715b1eaf9c1793b9a8c7b1787e2d6511ca2b004">debiman
now uses</a> for significantly faster manpage conversion (useful for disaster
recovery/development). An entire run previously took 2 hours on my workstation.
With this change, it takes merely 22 minutes. The effects are even more
pronounced on manziarly, the VM behind manpages.debian.org.
</li>
<li>
Thanks to Peter Palfrader (weasel) from the Debian System Administrators (DSA)
team, manpages.debian.org is now serving its manpages (and most of its
redirects) from Debian’s static mirroring infrastructure. That way, planned
maintenance won’t result in service downtime. I contributed <a
href="https://anonscm.debian.org/git/mirror/dsa-puppet.git/tree/modules/roles/README.static-mirroring.txt">README.static-mirroring.txt</a>,
which describes the infrastructure in more detail.
</li>
</ul>

<p>
The list above is not complete, but rather a selection of things I found worth
pointing out to the larger public.
</p>

<p>
There are still a few things I plan to work on soon, so stay tuned :).
</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Debian stretch on the Raspberry Pi 3 (update)]]></title>
    <link href="https://michael.stapelberg.ch/posts/2017-03-22-raspberry-pi-3/"/>
    <id>https://michael.stapelberg.ch/posts/2017-03-22-raspberry-pi-3/</id>
    <published>2017-03-22T18:36:00+02:00</published>
    <updated>2019-02-04T19:11:20+01:00</updated>
    <content type="html"><![CDATA[<p>
I previously wrote about <a
href="https://people.debian.org/~stapelberg/2016/11/24/raspberry-pi-3.html">my
Debian stretch preview image for the Raspberry Pi 3</a>.
</p>

<p>
Now, I’m publishing an updated version, containing the following changes:
</p>
<ul>
<li>
A new version of the upstream firmware makes the Ethernet MAC address persist
across reboots.
</li>
<li>
Updated initramfs files (without updating the kernel) are now correctly copied
to the VFAT boot partition.
</li>
<li>
The initramfs’s file system check now works as the required fsck binaries are
now available.
</li>
<li>
The root file system is now resized to fill the available space of the SD card
on first boot.
</li>
<li>
SSH access is now enabled, restricted via iptables to local network source
addresses only.
</li>
<li>
The image uses the linux-image-4.9.0-2-arm64 4.9.13-1 kernel.
</li>
</ul>

<p>
A couple of issues remain, notably the lack of HDMI, WiFi and bluetooth support
(see <a
href="https://wiki.debian.org/RaspberryPi3">wiki:RaspberryPi3</a> for details.
Any help with fixing these issues is very welcome!
</p>

<p>
As a <strong>preview version</strong> (i.e. unofficial, unsupported, etc.)
until all the necessary bits and pieces are in place to build images in a
proper place in Debian, I built and uploaded the resulting image. Find it at <a
href="https://people.debian.org/~stapelberg/raspberrypi3/2017-03-22/">https://people.debian.org/~stapelberg/raspberrypi3/2017-03-22/</a>.
To install the image, insert the SD card into your computer (I’m assuming it’s
available as <code>/dev/sdb</code>) and copy the image onto it:
</p>

<pre>
$ wget https://people.debian.org/~stapelberg/raspberrypi3/2017-03-22/2017-03-22-raspberry-pi-3-stretch-PREVIEW.img
$ sudo dd if=2017-03-22-raspberry-pi-3-stretch-PREVIEW.img of=/dev/sdb bs=5M
</pre>

<p>
If resolving client-supplied DHCP hostnames works in your network, you should
be able to log into the Raspberry Pi 3 using SSH after booting it:
</p>

<pre>
$ ssh root@rpi3
# Password is “raspberry”
</pre>
]]></content>
  </entry>
</feed>
